{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3029e100-8294-4f00-95df-09eb2c1df926",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is one part of the Data Investigation Process that can include data cleaning, wrangling, and visualization. The \"Data Moves\" framework:\n",
    "\n",
    "- Provides a structured set of categories (i.e., data moves) to describe and analyze how students engage with data\n",
    "\n",
    "- Supports instructional design and assessment by offering a lens through which educators can identify, understand, and demonstrate data practices\n",
    "\n",
    "Before exploring data, it is important to select datasets that are appropriate for students based on grade-level, subject area relevance, size, and number of freatures (i.e., variables). This notebook covers basic considerations that should be made before selecting datasets suitable for use in exploratory data analysis within an introductory data science course. It also provides examples of the core data moves along with explanations of output generated by each the move (e.g, a value, table, visualization, etc.).\n",
    "\n",
    "## Selecting a Dataset\n",
    "\n",
    "### Tidy Data\n",
    "\n",
    "The 2014 paper *Tidy Data* presents a structured framework for organizing datasets to support efficient analysis. In a tidy dataset, each variable forms a column, each observation forms a row, and each type of observational unit is stored in a separate table. It also outlines strategies for transforming messy data into tidy form, demonstrating how this approach simplifies and strengthens data analysis practices.\n",
    "\n",
    "Wickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n",
    "\n",
    "### Tame Data\n",
    "\n",
    "The 2018 paper The fivethirtyeight R Package introduces the concept of tame data, which refers to datasets that are clean, well-labeled, and easy to use in teaching. Tame data minimizes the need for wrangling so students can focus on analysis. The paper highlights the importance of using structured and accessible data in introductory statistics and data science courses.\n",
    "\n",
    "Kim, A. Y., Ismay, C., & Chunn, J. (2018). The fivethirtyeight R Package: “Tame Data” Principles for Introductory Statistics and Data Science Courses. Technology Innovations in Statistics Education, 11(1). https://doi.org/10.5070/T5111035892"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de5ad7-ef7e-4cd1-abc2-5b1745f3fb70",
   "metadata": {},
   "source": [
    "## Investigating Data Like a Data Scientist\n",
    "\n",
    "Investigating data like a data scientist involves an iterative process of making sense of information. This process includes six key phases: \n",
    "\n",
    "- Framing the problem\n",
    "- Exploring and visualizing data\n",
    "- Modeling\n",
    "- Evaluating results\n",
    "- Crafting a narrative\n",
    "- Communicating findings\n",
    "\n",
    "These phases reflect authentic data science practice and provide a structure that support more meaningful engagement with data in educational settings.\n",
    "\n",
    "Rather than following a fixed procedure, this framework emphasizes the importance of habits of mind such as critical thinking, refining questions, and considering the audience. It highlights that data science relies not only on technical skills but also on decision-making, interpretation, and storytelling. When students are guided through these phases, they develop the analytical reasoning needed to navigate data and communicate insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f700d5-9b19-402f-a43b-518b9eac5bce",
   "metadata": {},
   "source": [
    "## Data Investigation Process Framework\n",
    "\n",
    "\n",
    "### Frame Problem\n",
    "\n",
    "- Consider real-world phenomena and broader issues related to the problem\n",
    "- Pose investigative question(s)\n",
    "- Anticipate potential data and strategies\n",
    "\n",
    "### Consider & Gather Data\n",
    "\n",
    "- Understand possible attributes, measurements, and data collection methods needed for the problem\n",
    "- Evaluate and use appropriate design and techniques to collect or source data\n",
    "- Consider sample size, access, storage, and trustworthiness of data\n",
    "\n",
    "### Process Data\n",
    "\n",
    "- Organize, structure, clean, and transform data in efficient and useful ways\n",
    "- Consider additional data cases or attributes\n",
    "\n",
    "### Explore & Visualize Data\n",
    "\n",
    "- Construct meaningful visualizations, static or dynamic\n",
    "- Compute meaningful statistical measures\n",
    "- Explore and analyze data for potential relationships or patterns that address the problem\n",
    "\n",
    "### Consider Models\n",
    "\n",
    "- Analyze and identify models that address the problem\n",
    "- Consider assumptions and context of the models\n",
    "- Recognize possible limitations\n",
    "\n",
    "### Communicate & Propose Action\n",
    "\n",
    "- Craft a data story to convey insight to stakeholder audiences\n",
    "- Justify claims with evidence from data and propose possible action\n",
    "- Address uncertainty, constraints, and potential bias in the analysis\n",
    "\n",
    "Lee, H. S., Wilkerson, M. H., & Zuckerman, S. J. (2022). Investigating data like a data scientist: A framework for elementary, middle, and high school teachers. *Statistics Education Research Journal, 21*(2). [https://doi.org/10.52041/serj.v21i2.41](https://doi.org/10.52041/serj.v21i2.41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f8e39-2cac-4cbe-a3de-a1bab5569fe1",
   "metadata": {},
   "source": [
    "# Data Moves\n",
    "\n",
    "Data moves are strategic actions taken during data analysis to reshape and prepare datasets for interpretation. These include filtering, grouping, summarizing, calculating, merging or joining data, and creating hierarchical structures. Each move alters the structure, content, or values of a dataset, influencing what patterns become visible and what questions can be explored. By understanding these moves, learners gain insight into how data analysis is an active, decision-driven process rather than a passive application of procedures.\n",
    "\n",
    "Erickson, T., Tinker, R., & Yasuda, M. (2019). *Data moves*. UC Berkeley: The Concord Consortium. eScholarship, University of California. https://escholarship.org/uc/item/0mg8m7g6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588b1a9-2729-4323-8ca2-7ffa2dd2c412",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "Merging combines multiple datasets into one. The simplest form of merging concatenates datasets about the same phenomenon but from different sources, for example, combining height data from two different classrooms to make a larger dataset. \n",
    "\n",
    "## Joining\n",
    "\n",
    "Joining is a more complex form of merging. It does not add new cases, but\n",
    "rather adds more information (i.e., new attributes) about existing cases from a\n",
    "separate dataset. For example, in a school system, student demographic data might be stored in one table and test scores in another. Using a student ID as a key, the two tables can be joined to combine information for the same students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753976c-790e-4597-85a4-7ed22449e3e3",
   "metadata": {},
   "source": [
    "# Data Dive\n",
    "\n",
    "A data dive is a focused exploratory analysis where students work closely with a dataset to uncover patterns, trends, and relationships by applying key data moves. For example, using a dataset about school lunch nutrition, students might begin by filtering to isolate meals served in a specific year or location. They could group the data by food category such as fruits, grains, or proteins to explore how nutritional content differs across types. Through summarizing, they might calculate the average number of calories or the typical sodium content for each group. Calculating might involve creating new variables, such as calories per gram or the percentage of a recommended daily intake. If additional data sources are provided, students could join datasets, such as connecting lunch menus with student demographic information, to add context and depth to their findings. These data moves help students make sense of multivariable datasets and support evidence-based insights.\n",
    "\n",
    "## Analysis with Data Moves in Python\n",
    "\n",
    "In this section, we present example use cases that demonstrate data moves using the Python programming language. While Python includes built-in tools and data structures for general data handling, it does not include a built-in data structure specifically designed for working with tidy data as defined by the _Tidy Data_ paper. To support the tidy data format and organize the analysis around data moves such as filtering, grouping, and summarizing, we will use the Pandas library for data wrangling, Numpy for scientific computing, and the Matplotlib library for creating visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba0a40-3b44-4635-9616-fbf8adfca33a",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f49403-b64c-445b-8e65-26afa6ef46bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18028f5-f89a-49bd-8db2-1617ab7791f7",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1e3eb-c65b-47fa-934f-1a2ebb9630c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Popular Baby Names Dataset\n",
    "\n",
    "This dataset contains state-specific data on the relative frequency of given names for individuals issued a Social Security Number in the United States. Data is tabulated from Social Security Administration records as of March 2, 2025. The files include annual birth name frequencies by sex and state, beginning in 1910, for all 50 states and the District of Columbia.\n",
    "\n",
    "Each file lists names with at least 5 occurrences in a given year to protect individual privacy. Records are sorted by sex, year, and descending frequency, with alphabetical order breaking ties, which enables direct rank determination.\n",
    "\n",
    "#### Popular Baby Names Datasheet\n",
    "\n",
    "[Popular Baby Names Datasheet](https://docs.google.com/document/d/1uMFpRbvO1NhGVvfRSw3eDpp1O-6a7UeLE3GeZ8OfTuM/edit?usp=sharing)\n",
    "\n",
    "Social Security Administration. (n.d.). Popular baby names: Data limits and exclusions. Retrieved March 2, 2025, from https://www.ssa.gov/oact/babynames/limits.html\n",
    "\n",
    "The code below loads a subset of state files into separate dataframes. The table that follows shows how each file corresponds to its respective dataframe.\n",
    "\n",
    "|State|Abbreviation|\n",
    "|:-----|:------------|\n",
    "|Indiana| `ind`|\n",
    "|Michigan| `mich`|\n",
    "|Ohio| `ohio`| \n",
    "|Pennsylvania| `penn`|\n",
    "|North Carolina| `nc`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c99ca6-9cd9-4c62-94ce-15fa7cbfb782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ind = pd.read_csv(\"data/IN.TXT\")\n",
    "mich = pd.read_csv(\"data/MI.TXT\")\n",
    "ohio = pd.read_csv(\"data/OH.TXT\")\n",
    "penn = pd.read_csv(\"data/PA.TXT\")\n",
    "nc = pd.read_csv(\"data/NC.TXT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c794ce0-876b-404e-99d0-7d4c171599c3",
   "metadata": {},
   "source": [
    "It’s good practice to inspect the structure and metadata of a `DataFrame` using the `.info()` method.\n",
    "\n",
    "**Structure** refers to the overall layout of the `DataFrame`, including:\n",
    "- Number of rows and columns\n",
    "- Column names\n",
    "- Data types (e.g., int64, object, float64)\n",
    "- Index type and range\n",
    "\n",
    "**Metadata** refers to information about the data rather than the data itself, including:\n",
    "- Which columns contain missing values (non-null counts)\n",
    "- Total memory usage\n",
    "- Index type\n",
    "\n",
    "**Example 1.** Run the cell below. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc766d-fcfc-4374-925b-97e28e544171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ohio.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968937d2-0747-4446-a857-1204b283c56f",
   "metadata": {},
   "source": [
    "**Output Observations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d80d81-3918-4b9a-80dd-91d91ae26500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ohio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d3ab5-fc78-4867-9dcf-ae34b0b63502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reads the 'OH.TXT' file from the 'data' folder into a DataFrame named 'ohio'\n",
    "# header=None tells pandas that the file does not have a header row, so it should \n",
    "# not treat the first row as column names\n",
    "# Default column names will be assigned as integers: 0, 1, 2, 3, etc.\n",
    "ohio = pd.read_csv(\"data/OH.TXT\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ea617-81c2-49a3-b816-da2ce810fd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ohio.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5b5fa-243f-4018-9643-904b642a561a",
   "metadata": {},
   "source": [
    "**Example 2.** Rename the columns of the `ohio` `DataFrame` as `state`, `sex`, `year`, `name`, and `count`. Then, display all column names to confirm that the new column was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f0355-835d-4c5a-9122-f2e8e23df35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigns custom column names to the 'ohio' DataFrame\n",
    "# These names replace the default numeric column \n",
    "# labels (0, 1, 2, 3, 4)\n",
    "ohio.columns = ['state', 'sex', 'year', 'name', 'count']\n",
    "ohio.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8250fec-d636-45f1-b57c-cfce42b111fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ohio.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d947c-7459-4398-b06f-35ca3bbc10a4",
   "metadata": {},
   "source": [
    "**Example 3.** Filter the `ohio` `DataFrame` for the name \"Alexa\" to see how its popularity has changed overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d499ec-4591-4e5f-8e54-f421a9bd53b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Alexa\"\n",
    "\n",
    "# Creates a Boolean mask that is True for rows where the 'name' column matches the variable 'name'\n",
    "mask = ohio[\"name\"] == name\n",
    "\n",
    "# Uses the mask to filter the DataFrame and return only the matching rows\n",
    "# Then selects the 'year' and 'count' columns using double brackets [[...]] to return a DataFrame\n",
    "ohio[mask][[\"year\", \"count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27ea4a-6c7b-4a25-99d4-3842fed8f014",
   "metadata": {},
   "source": [
    "**Example 4.** Create a line chart to visualize how the popularity of the name \"Alexa\" has changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a3b1f-906c-4e2e-a835-9752f15b9650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ohio[ohio[\"name\"] == name].plot(x=\"year\", y=\"count\", kind=\"line\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8038a48-7bd6-497f-b00c-8dc763a4f4b8",
   "metadata": {},
   "source": [
    "**Output Observations:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f4c5e-a736-401b-9a2f-344b39f09e94",
   "metadata": {},
   "source": [
    "**Example 5.** Merge the Michigan data with the Ohio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320922c5-f0f5-40db-a6e1-4ddc50b1c7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mich = pd.read_csv(\"data/MI.TXT\", header=None)\n",
    "mich.columns = ['state', 'sex', 'year', 'name', 'count']\n",
    "mich.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb40cf-71ba-4786-acc7-b4a134b0905c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combines the 'ohio' and 'mich' DataFrames into a single DataFrame using pd.concat()\n",
    "# ignore_index=True resets the row index so it runs from 0 to n-1 in the combined DataFrame\n",
    "pd.concat([ohio, mich], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d919c-83ea-4e04-849b-74b36bcc8cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stores the combined data from both Ohio and Michigan in a DataFrame\n",
    "ohio_mich = pd.concat([ohio, mich], ignore_index=True)\n",
    "\n",
    "# Displays the combined DataFrame with data from both Ohio and Michigan\n",
    "# By default the first 5 and the last 5 rows are shown\n",
    "ohio_mich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fb99f-f3ed-4456-8e8c-f934fd9648da",
   "metadata": {},
   "source": [
    "### Joining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b53a48-7c68-467e-ac6a-276ee6ddfd4c",
   "metadata": {},
   "source": [
    "### CEO Compensation Summary Dataset\n",
    "\n",
    "The data from the AFL-CIO Executive Paywatch database draws from company proxy statements that are filed with the U.S. Securities and Exchange Commission and collected by [pay-gap.com](https://aflcio.org/paywatch/pay-gap.com). The database includes data for some 3,000 corporations, including most of those listed in the Russell 3000 Index. Industry classifications are based on North American Industry Classification System codes.\n",
    "\n",
    "#### CEO Compensation Summary Datasheet\n",
    "\n",
    "[CEO Compensation Summary Datasheet](https://docs.google.com/document/d/1AJriZiqMarx8-r4WZwoXoCkFKLEDpq2jt0V6t5_tQLM/edit?usp=drive_link)\n",
    "\n",
    "AFL-CIO. (n.d.). Highest-Paid CEOs. Retrieved 2022, from https://aflcio.org/paywatch/highest-paid-ceos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd758267-b93f-481a-883f-d563786dd508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ceo = pd.read_csv(\"data/ceo_compensation_summary.csv\")\n",
    "ceo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53f00f-c8e8-4071-bf83-4ac12cbb06b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ceo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba622c1-b668-4312-8c11-ed48d42c4267",
   "metadata": {},
   "source": [
    "### Compnay Information Dataset\n",
    "\n",
    "The companies in this dataset come from the AFL-CIO Executive Paywatch database, which compiles data from company proxy statements filed with the U.S. Securities and Exchange Commission and collected by paygap.com. The dataset includes approximately 3,000 corporations, primarily those listed in the Russell 3000 Index, with industry classifications based on North American Industry Classification System (NAICS) codes. To supplement this dataset, additional company information including sector, industry, and market capitalization was collected using the Python yfinance library, which provides streamlined access to company data from Yahoo Finance.\n",
    "\n",
    "#### Compnay Information Datasheet\n",
    "\n",
    "[Compnay Information Datasheet](https://docs.google.com/document/d/1t_J1RKSpc8qXhozS8F1K82Ac-429zETQUJXT8PvRCm0/edit?usp=sharing)\n",
    "\n",
    "Ran, A. (2019). yfinance: Yahoo! Finance market data downloader [Python library]. https://github.com/ranaroussi/yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc31649-ba8d-407d-9a5b-30b326053f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = pd.read_csv(\"data/company_information.csv\")\n",
    "company.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a36f1e-a6cf-4b97-a50c-4bd72b366d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "company.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4e8f0-040a-42ad-abb5-3b5e8da5ae14",
   "metadata": {},
   "source": [
    "**Example 6.** Display the column names from the both the `ceo` and the `company` `DtatFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb0439-b203-4c13-9319-176d3d974c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Colimns in the ceo dataframe\")\n",
    "print(ceo.columns)\n",
    "print(\"\\n\")\n",
    "print(\"Columns in the company dataframe\")\n",
    "print(company.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c63293-a462-4a46-8176-3b0bddfd4207",
   "metadata": {},
   "source": [
    "**Example 7.** Display the information in the first row of the `ceo` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f8a48-7794-4e29-b578-b4ba01a6814e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .loc is a label-based accessor used to retrieve rows (and optionally columns) by index label\n",
    "# This line retrieves the row in the 'ceo' DataFrame with index label 0\n",
    "# It returns all column values for that row as a Series\n",
    "ceo.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82525158-e49f-4f05-b241-3eb6ab7e5440",
   "metadata": {},
   "source": [
    "**Example 8.** Display the information in the first row of the `company` `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ee5a0-7645-41bd-bef3-fc965a4d43d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .loc is a label-based accessor used to retrieve rows (and optionally columns) by index label\n",
    "# This line retrieves the row in the 'company' DataFrame with index label 0\n",
    "# It returns all column values for that row as a Series\n",
    "company.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b9843-b80c-49e4-8a7e-1d1c231b41f2",
   "metadata": {},
   "source": [
    "**Example 9.** Use `pd.merge()` to combine the `ceo` and `company` dataframes based on the shared `ticker` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ebe57-c83d-4058-9ae4-1a3bac85f112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merges the 'ceo' and 'company' DataFrames using the 'ticker' column as the key\n",
    "# Only rows with matching 'ticker' values in both DataFrames will be included (inner join by default)\n",
    "# Returns a new DataFrame that combines columns from both sources\n",
    "pd.merge(ceo, company, on=\"ticker\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
