{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3029e100-8294-4f00-95df-09eb2c1df926",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is one part of the Data Investigation Process that can include data cleaning, wrangling, and visualization. The \"Data Moves\" framework:\n",
    "\n",
    "- Provides a structured set of categories (i.e., data moves) to describe and analyze how students engage with data\n",
    "\n",
    "- Supports instructional design and assessment by offering a lens through which educators can identify, understand, and demonstrate data practices\n",
    "\n",
    "Before exploring data, it is important to select datasets that are appropriate for students based on grade-level, subject area relevance, size, and number of freatures (i.e., variables). This notebook covers basic considerations that should be made before selecting datasets suitable for use in exploratory data analysis within an introductory data science course. It also provides examples of the core data moves along with explanations of output generated by each the move (e.g, a value, table, visualization, etc.).\n",
    "\n",
    "## Selecting a Dataset\n",
    "\n",
    "### Tidy Data\n",
    "\n",
    "The 2014 paper *Tidy Data* presents a structured framework for organizing datasets to support efficient analysis. In a tidy dataset, each variable forms a column, each observation forms a row, and each type of observational unit is stored in a separate table. It also outlines strategies for transforming messy data into tidy form, demonstrating how this approach simplifies and strengthens data analysis practices.\n",
    "\n",
    "Wickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10), 1–23. https://doi.org/10.18637/jss.v059.i10\n",
    "\n",
    "### Tame Data\n",
    "\n",
    "The 2018 paper The fivethirtyeight R Package introduces the concept of tame data, which refers to datasets that are clean, well-labeled, and easy to use in teaching. Tame data minimizes the need for wrangling so students can focus on analysis. The paper highlights the importance of using structured and accessible data in introductory statistics and data science courses.\n",
    "\n",
    "Kim, A. Y., Ismay, C., & Chunn, J. (2018). The fivethirtyeight R Package: “Tame Data” Principles for Introductory Statistics and Data Science Courses. Technology Innovations in Statistics Education, 11(1). https://doi.org/10.5070/T5111035892"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de5ad7-ef7e-4cd1-abc2-5b1745f3fb70",
   "metadata": {},
   "source": [
    "## Investigating Data Like a Data Scientist\n",
    "\n",
    "Investigating data like a data scientist involves an iterative process of making sense of information. This process includes six key phases: \n",
    "\n",
    "- Framing the problem\n",
    "- Exploring and visualizing data\n",
    "- Modeling\n",
    "- Evaluating results\n",
    "- Crafting a narrative\n",
    "- Communicating findings\n",
    "\n",
    "These phases reflect authentic data science practice and provide a structure that support more meaningful engagement with data in educational settings.\n",
    "\n",
    "Rather than following a fixed procedure, this framework emphasizes the importance of habits of mind such as critical thinking, refining questions, and considering the audience. It highlights that data science relies not only on technical skills but also on decision-making, interpretation, and storytelling. When students are guided through these phases, they develop the analytical reasoning needed to navigate data and communicate insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f700d5-9b19-402f-a43b-518b9eac5bce",
   "metadata": {},
   "source": [
    "## Data Investigation Process Framework\n",
    "\n",
    "\n",
    "### Frame Problem\n",
    "\n",
    "- Consider real-world phenomena and broader issues related to the problem\n",
    "- Pose investigative question(s)\n",
    "- Anticipate potential data and strategies\n",
    "\n",
    "### Consider & Gather Data\n",
    "\n",
    "- Understand possible attributes, measurements, and data collection methods needed for the problem\n",
    "- Evaluate and use appropriate design and techniques to collect or source data\n",
    "- Consider sample size, access, storage, and trustworthiness of data\n",
    "\n",
    "### Process Data\n",
    "\n",
    "- Organize, structure, clean, and transform data in efficient and useful ways\n",
    "- Consider additional data cases or attributes\n",
    "\n",
    "### Explore & Visualize Data\n",
    "\n",
    "- Construct meaningful visualizations, static or dynamic\n",
    "- Compute meaningful statistical measures\n",
    "- Explore and analyze data for potential relationships or patterns that address the problem\n",
    "\n",
    "### Consider Models\n",
    "\n",
    "- Analyze and identify models that address the problem\n",
    "- Consider assumptions and context of the models\n",
    "- Recognize possible limitations\n",
    "\n",
    "### Communicate & Propose Action\n",
    "\n",
    "- Craft a data story to convey insight to stakeholder audiences\n",
    "- Justify claims with evidence from data and propose possible action\n",
    "- Address uncertainty, constraints, and potential bias in the analysis\n",
    "\n",
    "Lee, H. S., Wilkerson, M. H., & Zuckerman, S. J. (2022). Investigating data like a data scientist: A framework for elementary, middle, and high school teachers. *Statistics Education Research Journal, 21*(2). [https://doi.org/10.52041/serj.v21i2.41](https://doi.org/10.52041/serj.v21i2.41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f8e39-2cac-4cbe-a3de-a1bab5569fe1",
   "metadata": {},
   "source": [
    "# Data Moves\n",
    "\n",
    "Data moves are strategic actions taken during data analysis to reshape and prepare datasets for interpretation. These include filtering, grouping, summarizing, calculating, merging or joining data, and creating hierarchical structures. Each move alters the structure, content, or values of a dataset, influencing what patterns become visible and what questions can be explored. By understanding these moves, learners gain insight into how data analysis is an active, decision-driven process rather than a passive application of procedures.\n",
    "\n",
    "Erickson, T., Tinker, R., & Yasuda, M. (2019). *Data moves*. UC Berkeley: The Concord Consortium. eScholarship, University of California. https://escholarship.org/uc/item/0mg8m7g6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba03fb8-09a6-4040-a88a-e02827c9b1f1",
   "metadata": {},
   "source": [
    "## Calculating\n",
    "\n",
    "Calculating is the process of create=ing a new attribute, often represented by a new column in a data table. This typically involves calculating the values in this new attribute using a formula.\n",
    "\n",
    "- Many new attributes are calculated using the values from one or more existing attributes. \n",
    "- Summary measures function as new, conceptual attributes as well; the difference is that they appear on group labels rather than individual data cases.\n",
    "\n",
    "In addition to conceptual attributes, calculating can also be used to create\n",
    "convenience attributes. For example, one may wish to create a categorical\n",
    "attribute whose value is “tall” if an individual’s height is greater than the\n",
    "mean height for their age, and “short” otherwise. Convenience attributes are quite common. Other examples include:\n",
    "\n",
    "- Creating a new column that converts heights to inches instead of centimeters.\n",
    "- Using birth dates included in a dataset to compute subjects’ ages.\n",
    "- Recoding an education attribute from several categories (e.g., \"GED,\" \"high-school graduate,\" \"one year of college,\" \"bachelor’s degree,\" etc.) to fewer (perhaps, \"completed high school,\" \"completed college\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753976c-790e-4597-85a4-7ed22449e3e3",
   "metadata": {},
   "source": [
    "# Data Dive\n",
    "\n",
    "A data dive is a focused exploratory analysis where students work closely with a dataset to uncover patterns, trends, and relationships by applying key data moves. For example, using a dataset about school lunch nutrition, students might begin by filtering to isolate meals served in a specific year or location. They could group the data by food category such as fruits, grains, or proteins to explore how nutritional content differs across types. Through summarizing, they might calculate the average number of calories or the typical sodium content for each group. Calculating might involve creating new variables, such as calories per gram or the percentage of a recommended daily intake. If additional data sources are provided, students could join datasets, such as connecting lunch menus with student demographic information, to add context and depth to their findings. These data moves help students make sense of multivariable datasets and support evidence-based insights.\n",
    "\n",
    "## Analysis with Data Moves in Python\n",
    "\n",
    "In this section, we present example use cases that demonstrate data moves using the Python programming language. While Python includes built-in tools and data structures for general data handling, it does not include a built-in data structure specifically designed for working with tidy data as defined by the _Tidy Data_ paper. To support the tidy data format and organize the analysis around data moves such as filtering, grouping, and summarizing, we will use the Pandas library for data wrangling, Numpy for scientific computing, and the Matplotlib library for creating visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba0a40-3b44-4635-9616-fbf8adfca33a",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f49403-b64c-445b-8e65-26afa6ef46bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8741ee1-ddbd-4df1-bb29-f2bbd529e22a",
   "metadata": {},
   "source": [
    "### Updated Starwars Dataset\n",
    "\n",
    "Fabricio Narcizo’s blog post, Introduction to Data Analysis using the Star Wars Dataset, presents an expanded version of the original R dplyr Star Wars dataset, growing it from 14 to 25 variables. By cross-referencing Wookieepedia, he corrected and enriched the character data with new fields like birth/death details, pronouns, occupations, and abilities, resulting in a more accurate and comprehensive dataset for analysis.\n",
    "\n",
    "#### Updated Starwars Datasheet\n",
    "\n",
    "[Updated Starwars Datasheet](https://docs.google.com/document/d/1Gr6W0xo1pxW-TZH2GKawIASqZ7JfURmzQO1Eg2JGsC8/edit?usp=sharing)\n",
    "\n",
    "Narcizo, F. B. (2023, December 30). Introduction to Data Analysis using the Star Wars Dataset. Retrieved from https://www.fabricionarcizo.com/post/starwars/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3076ce3-0bb2-4b49-a50d-6552bd38faf9",
   "metadata": {},
   "source": [
    "**Example 1.** Load the dataset into a pandas `DataFrame` called `starwars`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34680e91-0c05-4f70-a02e-68e20c916ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "starwars = pd.read_csv(\"data/updated_starwars.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564726e6-5a72-412c-b4f8-b3138d7ce5c7",
   "metadata": {},
   "source": [
    "### Calculating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20ca47-912f-4f0c-9041-30f3e3814be5",
   "metadata": {},
   "source": [
    "**Example 2.** Convert all the heights in the `starwars` dataframe from meters to centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb74315-373e-41b1-85d6-e64446cbdaf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converts the 'height' column from centimeters to meters\n",
    "# Divides each value in the 'height' column by 100\n",
    "# This returns a Series of height in meters for each character\n",
    "starwars[\"height\"] / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9908053-836e-45e3-88c9-3ba4a685ea4c",
   "metadata": {},
   "source": [
    "**Example 3.** Create a new column in the `starwars` `DataFrame` called `height_m` that stores each character's height in meters. Then, display all column names to confirm that the new column was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff8615-7183-43d2-b56f-f671b5c86297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creates a new column called 'height_m' by converting height from centimeters to meters\n",
    "starwars[\"height_m\"] = starwars[\"height\"] / 100\n",
    "\n",
    "# Displays the names of all columns to confirm the new column was added\n",
    "starwars.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40fc7c-2d97-4044-8174-e3887fe03864",
   "metadata": {},
   "source": [
    "**Example 4.** Calculate the Body Mass Index (BMI) for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11d705-c793-4338-824f-7b9cb5d45066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculates BMI using the formula: mass (kg) divided by height (m) squared\n",
    "# 'mass' in kilograms\n",
    "# 'height_m' in meters\n",
    "# This returns a Series of BMI values for each character\n",
    "starwars[\"mass\"] / (starwars[\"height_m\"]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f5b088-8f44-4a15-8eda-b204f4b9238b",
   "metadata": {},
   "source": [
    "**Example 5.** Create a new column called `bmi` in the `starwars` `DataFrame` by calculating the Body Mass Index (BMI) for each character. Then display the column names to confirm the new column was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c9aa5-c690-4620-8cbc-44022d80f9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "starwars[\"bmi\"] = starwars[\"mass\"] / (starwars[\"height_m\"]**2)\n",
    "starwars.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510b8fb-d585-4f2f-b4c9-544898952ed2",
   "metadata": {},
   "source": [
    "**Example 6.** Create a list that classifies each character as either `\"Human\"`, `\"Droid\"`, or `\"Other\"` based on the value in the `species` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c26e557-c387-400a-b0f0-358c56d5a028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store the new species labels\n",
    "species = []\n",
    "\n",
    "# Loop through each value in the 'species' column\n",
    "for label in starwars[\"species\"]:\n",
    "    \n",
    "    # If the species is Human or Droid, keep the original label\n",
    "    if label in [\"Human\", \"Droid\"]:\n",
    "        species.append(label)\n",
    "    \n",
    "    # If the species is something else, label it as \"Other\"\n",
    "    else:\n",
    "        species.append(\"Other\")\n",
    "\n",
    "# Print the final list of species labels\n",
    "print(species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6291b8-baf8-4470-9b1a-c15bd4d34424",
   "metadata": {},
   "source": [
    "**Example 7.** Use the `.isin()` method to filter the the `starwars` `DataFrame` to identify which rows correspond to characters whose species is either Human or Droid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e62f50-4367-492c-bac4-6d6a84a58aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checks if the value in the 'species' column is either \"Human\" or \"Droid\"\n",
    "# .isin() returns True if the species is in the list, otherwise False\n",
    "# This returns a Series of True/False values\n",
    "starwars[\"species\"].isin([\"Human\", \"Droid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59b1ad-d171-472d-9fde-2261f7e3f766",
   "metadata": {},
   "source": [
    "**Example 8.** Use `np.where` to create an array that classifies each character as either `\"Human\"`, `\"Droid\"`, or `\"Other\"` based on the value in the species column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b916f-a44a-4bd2-ad8d-0bd66ef96dba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uses np.where to classify species as \"Human\", \"Droid\", or \"Other\"\n",
    "# The first argument checks if each species is either \"Human\" or \"Droid\"\n",
    "# The second argument keeps the original species if the condition is True\n",
    "# The third argument replaces all other species with \"Other\"\n",
    "# This returns a numpy array with values\n",
    "np.where(starwars[\"species\"].isin([\"Human\", \"Droid\"]), starwars[\"species\"], \"Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1a826-e5e7-4025-a000-85ea727b2911",
   "metadata": {},
   "source": [
    "`group_species` is a user-defined function that classifies a species value as either `\"Human\"`, `\"Droid\"`, or `\"Other\"`. It takes a single input, `s`, which must be a string of `None`. If the input is `\"Human\"` or `\"Droid\"`, the function returns that same value. If the input is anything other species name or missing values, it returns `\"Other\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc5a14-5ae8-4668-9d8b-124f2dd0be0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_species(s):\n",
    "    \"\"\"\n",
    "    Classifies a species value as 'Human', 'Droid', or 'Other'.\n",
    "    \n",
    "    Examples:\n",
    "    \n",
    "    group_species(\"Human\")  returns \"Human\"\n",
    "    group_species(\"Rodian\") returns \"Other\"\n",
    "    group_species(\"Droid\")  returns \"Droid\"\n",
    "\n",
    "    Parameters:\n",
    "    s: A single species value from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    str: Returns 'Human' or 'Droid' if the input matches those values. \n",
    "         Otherwise returns 'Other'.\n",
    "         \n",
    "    Precondition: s is a string or None\n",
    "    \"\"\"\n",
    "    if s in [\"Human\", \"Droid\"]:\n",
    "        return s\n",
    "    else:\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0be79-1b98-4ec4-b6d0-3e046acd03e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# .apply() runs the group_species function on each value in the 'species' column\n",
    "# It goes through the column one row at a time\n",
    "# This returns a Series of values of 'Human', 'Droid', or 'Other'\n",
    "starwars[\"species\"].apply(group_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a74ab-636f-463d-9655-7eb4fa427a74",
   "metadata": {},
   "source": [
    "You can use any of the previously demonstrated programming techniques to add a column to the `starwars` `DataFrame`. \n",
    "\n",
    "```python\n",
    "starwars[\"species_grps\"] = species\n",
    "starwars[\"species_grps\"] = starwars[\"species\"].apply(group_species)\n",
    "starwars[\"species_grps\"] = np.where(starwars[\"species\"].isin([\"Human\", \"Droid\"]), starwars[\"species\"], \"Other\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a3578-859c-47eb-a384-3eb8ded08204",
   "metadata": {},
   "source": [
    "**Example 24.** Choose a technique to add a new column to the `starwars` `DataFrame` that classifies each character as `\"Human\"`, `\"Droid\"`, or `\"Other\"` based on their species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16dc51-e2d9-4a58-b1a7-11b96aabe506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "starwars[\"species_grps\"] = ...\n",
    "starwars.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
